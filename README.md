# ğŸŒ¿ Agente Manuelita - Asistente Inteligente

Asistente inteligente con memoria conversacional, enrutamiento dinÃ¡mico y bÃºsqueda RAG para la empresa Manuelita.

## ğŸ¯ CaracterÃ­sticas

- **Enrutamiento Inteligente**: Detecta automÃ¡ticamente si usar RAG, herramientas estructuradas o memoria conversacional
- **Memoria Conversacional**: Mantiene contexto de mÃºltiples conversaciones
- **Sistema RAG**: BÃºsqueda semÃ¡ntica en documentos corporativos con ChromaDB
- **Multi-LLM**: Soporte para OpenAI, Google Gemini y Ollama (local)
- **Interfaz Streamlit**: UI moderna y personalizable con colores corporativos
- **Streaming de Respuestas**: Experiencia de chat fluida con efectos visuales

## ğŸ“ Estructura del Proyecto

```
agente_manuelita/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/              # MÃ³dulo principal del agente
â”‚   â”‚   â”œâ”€â”€ agent.py        # Clase ManuelitaAgent
â”‚   â”‚   â”œâ”€â”€ memory.py       # Sistema de memoria conversacional
â”‚   â”‚   â””â”€â”€ rag.py          # Sistema RAG con ChromaDB
â”‚   â”œâ”€â”€ tools/              # Herramientas del agente
â”‚   â”‚   â”œâ”€â”€ structured_tool.py
â”‚   â”‚   â””â”€â”€ data/           # Datos estructurados (FAQ)
â”‚   â”œâ”€â”€ utils/              # Utilidades
â”‚   â”‚   â””â”€â”€ parser.py       # Parser de documentos
â”‚   â””â”€â”€ config.py           # ConfiguraciÃ³n centralizada
â”œâ”€â”€ tests/                  # Pruebas unitarias
â”œâ”€â”€ configs/                # Archivos de configuraciÃ³n
â”œâ”€â”€ data/                   # Datos y documentos
â”œâ”€â”€ logs/                   # Logs de la aplicaciÃ³n
â”œâ”€â”€ vectordb/               # Base de datos vectorial (ChromaDB)
â”œâ”€â”€ .streamlit/             # ConfiguraciÃ³n de Streamlit
â”œâ”€â”€ app.py                  # AplicaciÃ³n principal Streamlit
â”œâ”€â”€ pyproject.toml          # Dependencias y configuraciÃ³n
â”œâ”€â”€ requirements.txt        # Dependencias pip
â”œâ”€â”€ .env.example            # Ejemplo de variables de entorno
â””â”€â”€ README.md               # Este archivo
```

## ğŸš€ InstalaciÃ³n

### 1. Clonar el repositorio

```bash
git clone <url-del-repo>
cd agente_manuelita
```

### 2. Crear entorno virtual

```bash
python -m venv .venv

# En Windows
.venv\Scripts\activate

# En Linux/Mac
source .venv/bin/activate
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

O con uv (mÃ¡s rÃ¡pido):

```bash
uv pip install -e .
```

### 4. Configurar variables de entorno

Copia `.env.example` a `.env` y configura tus API keys:

```bash
cp .env.example .env
```

Edita `.env`:

```env
# OpenAI (recomendado)
OPENAI_API_KEY=tu_api_key_aqui

# O Google Gemini (alternativa)
GOOGLE_API_KEY=tu_api_key_aqui

# O Ollama (local, sin API key necesaria)
OLLAMA_BASE_URL=http://localhost:11434

# LangSmith (opcional - para observabilidad)
LANGCHAIN_TRACING_V2=false
LANGCHAIN_API_KEY=
LANGCHAIN_PROJECT=manuelita-agent
```

## ğŸ® Uso

### Ejecutar la aplicaciÃ³n Streamlit

```bash
streamlit run app.py
```

La aplicaciÃ³n se abrirÃ¡ en `http://localhost:8501`

### Uso programÃ¡tico

```python
from src.agent import ManuelitaAgent

# Inicializar agente
agent = ManuelitaAgent(provider="OpenAI")

# Hacer una pregunta
resultado = agent.process("Â¿CuÃ¡l es la historia de Manuelita?")

print(f"Respuesta: {resultado['answer']}")
print(f"Tool usado: {resultado['tool_used']}")
print(f"Fuentes: {resultado['sources']}")
```

## ğŸ§ª Testing

Ejecutar pruebas:

```bash
pytest tests/
```

Con cobertura:

```bash
pytest --cov=src tests/
```

## ğŸ”§ ConfiguraciÃ³n

Edita `src/config.py` para personalizar:

- **Modelo LLM**: OpenAI, Google Gemini, o Ollama
- **Temperatura**: Control de creatividad del modelo
- **ParÃ¡metros RAG**: Top-k, tamaÃ±o de chunks
- **Memoria**: LÃ­mites de tokens y turnos
- **UI**: TÃ­tulos, colores, velocidad de streaming

## ğŸŒ Proveedores LLM Soportados

### OpenAI
```python
config.llm.provider = "OpenAI"
config.llm.model = "gpt-4o-mini"  # o gpt-4, gpt-4-turbo, o1, etc.
```

### Google Gemini
```python
config.llm.provider = "Google Gemini"
config.llm.model = "gemini-2.0-flash"  # o gemini-1.5-pro, etc.
```

### Ollama (Local)
```python
config.llm.provider = "Ollama"
config.llm.model = "qwen3:4b"  # o llama3, mistral, etc.
```

## ğŸ“Š Funcionalidades

### 1. Sistema RAG
- BÃºsqueda semÃ¡ntica en documentos corporativos
- Embeddings con sentence-transformers
- Base de datos vectorial ChromaDB
- Reranking con BM25

### 2. Memoria Conversacional
- GestiÃ³n de mÃºltiples conversaciones
- LÃ­mites configurables de tokens
- FIFO automÃ¡tico
- Persistencia en sesiÃ³n

### 3. Enrutamiento DinÃ¡mico
El agente decide automÃ¡ticamente quÃ© herramienta usar:
- **RAG**: Para preguntas generales sobre la empresa
- **Structured**: Para preguntas especÃ­ficas de FAQ
- **Memory**: Para preguntas contextuales (nombres, referencias)

### 4. Interfaz Streamlit
- 3 pÃ¡ginas: FAQs, Admin, Chat
- DiseÃ±o corporativo personalizado
- Streaming de respuestas
- GestiÃ³n de conversaciones

## ğŸ”’ Seguridad

- No expone prompts internos
- SanitizaciÃ³n de respuestas
- ValidaciÃ³n de entrada
- Control de acceso a API keys

## ğŸ“ Logs

Los logs se guardan en la carpeta `logs/`:

```
logs/
â”œâ”€â”€ app.log
â”œâ”€â”€ agent.log
â””â”€â”€ rag.log
```

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea tu feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push al branch (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto es privado y propiedad de Manuelita.

## ğŸ‘¥ Autores

- **Esteban Arroyave LÃ³pez**
- **Manuel Luna AlegrÃ­a** 

**Desarrollado con â¤ï¸ pensando en Manuelita S.A., en el marco del curso Tecnicas avanzadas con LLM** ğŸŒ¿
